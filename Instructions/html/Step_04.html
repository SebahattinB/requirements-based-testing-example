
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>Step 4: Testing By Simulation</title><meta name="generator" content="MATLAB 9.6"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2019-04-08"><meta name="DC.source" content="Step_04.m"><style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:100%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:14px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:2.0em; color:#000077; line-height:150%; font-weight:bold; text-align:center }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.6em; color:#444444; font-weight:bold; font-style:italic; text-align:left; vertical-align:bottom; line-height:200%; border-top:2px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.1em; color:#000; font-weight:bold; line-height:140%; }

a { color:#555555; font-style:italic; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img, h1 img, h2 img { margin-bottom:0px;} 

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }

pre, code { font-size:12px; }
tt { font-size: 1.2em; }
pre { margin:0px 0px 20px; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }
pre.error { color:red; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }

.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; }
.footer a { color:#878787; }
.footer a:hover { color:#878787; text-decoration:underline; }
.footer a:visited { color:#878787; }

table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }





  </style></head><body><div class="content"><h1>Step 4: Testing By Simulation</h1><!--introduction--><!--/introduction--><h2>Contents</h2><div><ul><li><a href="#1">Introduction</a></li><li><a href="#2">Verification and Validation Tools Used</a></li><li><a href="#3">Simulink Test Overview</a></li><li><a href="#4">Test Harness Introduction</a></li><li><a href="#5">Creating a Test Harness at the Subsystem Level</a></li><li><a href="#6">Creating a Test Harness at the Model Level</a></li><li><a href="#7">Importing Test Vectors</a></li><li><a href="#8">Completing the Test Harness Model</a></li><li><a href="#9">Executing the Functional Tests and Analyzing the Results</a></li><li><a href="#10">Test Sequence Blocks for Inputs and Online Assessment</a></li><li><a href="#11">Simulink Test Manager Overview</a></li><li><a href="#12">Importing a Test Harness into the Test Manager</a></li><li><a href="#13">Introduction to the Model Coverage Concept</a></li><li><a href="#14">Collecting Model Coverage From Functional Test Cases</a></li><li><a href="#15">Using the Model Coverage Results</a></li><li><a href="#16">Summary</a></li></ul></div><h2>Introduction<a name="1"></a></h2><p><img vspace="5" hspace="5" src="Step_04_CruiseControl_WhatNow.png" alt=""> </p><p>The most common verification method is to actually execute the algorithm or program, i.e. so called dynamic testing. In Simulink this is done using simulation by simply pressing <b>PLAY</b>, or what we covered in the first step of the workshop, <b>Ad-hoc Testing</b>.  This step takes a more formalized, structured approach to dynamic testing, goes beyond ad-hoc testing to answer the next questions:</p><div><ul><li>Does the implementation pass all functional requirements?</li><li>Do the tests completely exercise the implementation?</li></ul></div><p>Dynamic testing involves creating a set of test vectors based on the functional requirements, and ensuring that the algorithm output meets the expected output. However, determining that the algorithm behaves as expected is only part of the overall test objectives. Another vital part is to get information about how much of the algorithm has been covered by the test cases. In Simulink, you can do this by enabling model coverage measurements during simulation. Model coverage is able to tell us whether or not we have achieved a "minimal" amount of testing. For example, if we have not reached 100% decision coverage, we have not exercised or tested 100% of our model.</p><p>In this section, we will see how we can quickly build up a test environment by creating a test harness model, capturing model coverage information and verify that our test results meet our expected outputs. We will use the test harness to debug the model when we fail the functional testing.  Model coverage results will also help the debugging effort but more fundamentally it will provide an additional check that the implementation matches the requirements beyond the previous traceability discussion.</p><p><img vspace="5" hspace="5" src="Step_04_SimTestWorklow.png" alt=""> </p><h2>Verification and Validation Tools Used<a name="2"></a></h2><div><ul><li>Simulation Data Inspector (part of Simulink)</li><li>Simulink Verification and Validation</li><li>Simulink Test</li></ul></div><h2>Simulink Test Overview<a name="3"></a></h2><p>In this step we will use the features of <b>Simulink Test</b> to perform the dynamic testing of our cruise controller.  <b>Simulink Test</b> supports interactive testing of the model with creation and management of <b>test harnesses</b>.  To create input vectors for use in the harness we will mostly use Signal Builder and a brief example of using a <b>test sequence</b> block from <b>Simulink Test</b>.  For evaluation we will introduce the use of a another instance of a <b>test sequence</b> block.  And lastly we will use the <b>test manager</b> of <b>Simulink Test</b> to automate the execution, evaluation and reporting of our results.</p><p><img vspace="5" hspace="5" src="Step_04_SLTestOverview.png" alt=""> </p><h2>Test Harness Introduction<a name="4"></a></h2><p>Simulink Verification and Validation contains several functions that help the user create test harness models to facilitate the dynamic functional testing.  The test harness provides an environment to:</p><div><ul><li>Import test cases</li><li>Link to requirements</li><li>Execute the tests</li><li>Evaluate the results</li><li>Debug the failed tests</li><li>Measure test completeness</li></ul></div><p>In the past, to create the test harness we would have used the <b>slvnvmakeharness</b> function provided in the <b>Simulink Verification and Validation</b> toolbox.  An identical function <b>sldvmakeharness</b> is provided with the <b>Simulink Design Verifier</b> toolbox.  But we will use the new <b>test harness</b> creation feature provided by <b>Simulink Test</b>.</p><p><img vspace="5" hspace="5" src="Step_04_SLTestHarnessOverview.png" alt=""> </p><h2>Creating a Test Harness at the Subsystem Level<a name="5"></a></h2><p>One of the features supporting algorithm development and testing is the ability to create a test harness at the subystem level.  In the past the algorithm developer would need to extract the subystem, manually create the harness and coordinate any changes between the main model and the harnessed subsystem.  With <b>Simulink Test</b> these steps are automated and managed.</p><p>Based on field testing of the Cruise Control, an issue was found by pressing both the "CoastSetSw" and the "AccelResSw" simultaneously.  The Cruise Control target speed was increased but this was not specified in the requirements, in fact it was missing. A new design was created to ignore the simultaneous input by implementing a subystem to condition the two inputs to be rejected.</p><p>To examine the design and create the subsystem harness, do the following:</p><p>1.  Open "CruiseControl.slx" with the subsystem redesign - <b><a href="matlab:C0_FuncTest_Sub;">click here</a></b>.</p><p><img vspace="5" hspace="5" src="Step_04_SLTestSubReDesignMain.png" alt=""> </p><p>2.  Open the "RejectDoublePress" subsystem to examine the design.</p><p><img vspace="5" hspace="5" src="Step_04_SLTestSubReDesign.png" alt=""> </p><p>3.  Navigate to the top model.  Right-click on the "RejectDoublePress" subsystem and select *</p><p><img vspace="5" hspace="5" src="Step_04_SLTestSubHarnessMenu.png" alt=""> </p><p>4.  Enter "RejectDoublePress_Wkshp_Harness" as the <b>Name</b> and select <b>Constant</b> as the source to create a harness to test the design interactively.</p><p><img vspace="5" hspace="5" src="Step_04_SLTestSubHarnessDlg.png" alt=""> </p><p>Once the harness has opened we can see the subsystem has been extracted from the main model.  To complete the harness we can add "dashboard" blocks to control the inputs and monitor the outputs.  The next step will detail the creation of a test harness. For this section, we will focus on the subsystem testing workflow with an existing interactive test harness.</p><p><img vspace="5" hspace="5" src="Step_04_SLTestSubHarnessNew.png" alt=""> </p><p>5.  Close the harness by closing the window, and open the existing "RejectDoublePress_Adhoc_Harness".</p><p><img vspace="5" hspace="5" src="Step_04_SLTestSubHarnessOpen.png" alt=""> </p><p>6.  Run the test harness to see if it behaves as intended.</p><p><img vspace="5" hspace="5" src="Step_04_SLTestSubHarnessResult.png" alt=""> </p><p>Based on the testing, the design is behaving exactly opposite to what was intended.  It is rejecting single inputs and letting simulataneous input pass through to "ComputeTargeSpeed".</p><p>7.  To fix the design, modify the "AND" block to a "NAND" block.  Check the modified design for intended behavior.</p><p><img vspace="5" hspace="5" src="Step_04_SLTestSubHarnessFix.png" alt=""> </p><p>8.  Close the harness and examine the contents of the main model.  The subystem contents have been updated based on the work done in the harness.</p><p><img vspace="5" hspace="5" src="Step_04_SLTestHarnessFixInMain.png" alt=""> </p><h2>Creating a Test Harness at the Model Level<a name="6"></a></h2><p>Now we will create a test harness at the model level.  To create the test harness model, do the following:</p><p>1.  Open <b>CruiseControl.slx</b> - <b><a href="matlab:loadFuncTestMdl;">click here</a></b>.</p><p>Examine the state chart to see the Cruise Control logic includes the bug fixes from our previous work:</p><div><ul><li>Exit condition fix based on work done in <b>Step 0: Ad-hoc Testing</b></li><li>Calculation order and calibration lower limit fix based on work done in <b>Step 3:  Detecting Design Errors</b></li></ul></div><p>2.  We will create a harness with a <b>Signal Builder</b> block for the inputs and a <b>test sequence</b> block for the assessment.  To begin the creation of the test harness, select <b>Analysis</b>, <b>Test Harness</b>, <b>Create Test Harness (Cruise Control) ...</b>.  If the menu selection for <b>Create Test Harness (ComputeTargetSpeed)...</b> appears, you need to de-select the <b>ComputeTargetSpeed</b> subsystem, otherwise you will be creating a harness for the subsystem instead of the model.</p><p><img vspace="5" hspace="5" src="Step_04_SLTestHarnessMenu.png" alt=""> </p><p>3.  In the "Create Test Harness" user interface, enter the name "CruiseControl_Harness_Short".</p><p>4.  Select "Signal Builder" as the source block and check "Add separate assessment block". Select "OK" when completed.</p><p><img vspace="5" hspace="5" src="Step_04_SLTestHarnessUI.png" alt=""> </p><p>The harness model consists of (6) components:</p><div><ol><li>Source: in this case a Signal Builder block</li><li>Input Conversion Subsystem: Contains datatype and rate transition blocks. Buses and Vectors are also handled within this subsystem (read-only)</li><li>Unit Under Test:  Model reference block containing Cruise Control model</li><li>Output Conversion Subsystem:  Contains datatype and rate transition blocks. Buses and Vectors are also handled within this subsystem (read-only)</li><li>Sink:  in this case the Outport blocks</li><li>Test Assessment Block:  Contains statements to compare the actual outputs to the expected outputs</li></ol></div><p><img vspace="5" hspace="5" src="Step_04_SLTestHarness.png" alt=""> </p><h2>Importing Test Vectors<a name="7"></a></h2><p>Based on our algorithm requirements, we have created a test plan document from which we have derived test vectors in an Excel file. This is a common way to define test cases in the industry. Next, we are going to populate the Signal Builder block with test vectors, including expected outputs, from the Excel file.</p><p>Do the following:</p><p>1.  Open the test plan document &#8211; <b><a href="matlab:winopen('cruise_control_testplan_short.docx')">click here</a></b>.</p><p><img vspace="5" hspace="5" src="Step_04_TestPlan.png" alt=""> </p><p>2.  Review the 6 test case descriptions.  This is only addressing a subset of the requirements for now.</p><p>From the test plan, test cases were created by interpreting the test plan into a set time based test vectors and expected results.  The test cases were entered into a spreadsheet. Each test case is in its own worksheet and the top labels are the same for all test cases. In a later section of this step we will use a larger set of test cases but for this introduction the smaller set will be more efficient.</p><p>3.  Open the Excel file that contains the test vectors - <b><a href="matlab:winopen('CruiseControlTests_short.xlsx')">click here</a></b>. For each of the 6 test cases described in the test plan document, there is a corresponding Excel sheet.</p><p><img vspace="5" hspace="5" src="Step_04_TestCases.png" alt=""> </p><p>4.  Open the <b>Signal Builder</b> block in the test harness and select <b>File</b>, and <b>Import from File</b>.</p><p><img vspace="5" hspace="5" src="Step_04_SB_ImportMenu.png" alt=""> </p><p>5. Under <b>Select file to import</b>, click <b>Browse</b>, navigate to the <b>Test</b> directory, select the <b>CruiseControlTests_short.xlsx</b> file.  <b>Import File</b> dialog will now have the <b>Files to Import</b> textbox populated.</p><p><img vspace="5" hspace="5" src="Step_04_SB_Import_FileSelect.png" alt=""> </p><p>6. After the data has been imported, check the <b>Select All</b>, <b>Replace existing dataset</b>, and <b>Confirm Selection options</b>, and then <b>Apply</b>.</p><p><img vspace="5" hspace="5" src="Step_04_SB_ImportUI.png" alt=""> </p><p>7. Select the <b>No, import without saving</b> option. The Signal Builder block should now have all the Excel data.</p><p><img vspace="5" hspace="5" src="Step_04_SB_ImportPopup.png" alt=""> </p><p><img vspace="5" hspace="5" src="Step_04_SB_TestCasesUI.png" alt=""> </p><h2>Completing the Test Harness Model<a name="8"></a></h2><p>Please note that during the import, the signal lines between the Signal Builder block and the Test Unit have been disconnected. We now need to setup the harness model for easily logging and inspecting of the simulation results. Please do the following:</p><p>1.  If necessary, reconnect lines from "Signal Builder" to the "Input Conversion Subsystem" block.</p><p>2.  For the expected output signals:  Add <b>Data Type Conversion</b> blocks, set the data type ("boolean" for <b>engaged</b> and "uint8" for <b>tspeed</b>).</p><p>3.  Label the signals to be <b>Exp_engaged</b> and <b>Exp_tspeed</b>.  Terminate expected signals with <b>Goto</b> blocks with tag names matching the signal names.</p><p><img vspace="5" hspace="5" src="Step_04_HarnessExpOutputs.png" alt=""> </p><p>4.  Open the "Test Assessment" block and remove all inputs except <b>engaged</b> and <b>tspeed</b>.</p><p><img vspace="5" hspace="5" src="Step_04_TestAssessDeleteInputs.png" alt=""> </p><p>5.  Add inputs for <b>Exp_engaged</b> and <b>Exp_tspeed</b>.</p><p><img vspace="5" hspace="5" src="Step_04_TestAssessAddInputs.png" alt=""> </p><p>6.  Uncheck the "When decomposition" to create a simple step type.</p><p><img vspace="5" hspace="5" src="Step_04_TestAssessRemoveWhen.png" alt=""> </p><p>7.  Remove all substeps.</p><p><img vspace="5" hspace="5" src="Step_04_TestAssessRemoveStep.png" alt=""> </p><p>8.  Add "verify" statements.</p><p><img vspace="5" hspace="5" src="Step_04_TestAssessment.png" alt=""> </p><p>9.  Remove all unconnected lines and <b>From</b> blocks to the "Test Assessment Block".  For the <b>Exp_engaged</b> and <b>Exp_tspeed</b> signals: Create <b>From</b> blocks and connect to "Test Assessment" block.</p><p>You should now have a harness model that looks like the picture below:</p><p><img vspace="5" hspace="5" src="Step_04_HarnessConnected.png" alt=""> </p><p>To log the signals for easy inspection, please do the following:</p><p>10.  Select the output signals: <b>engaged</b>, <b>tspeed</b>, <b>Exp_engaged</b>, and <b>Exp_tspeed</b>.  And select the input signals: <b>CruiseOnOff</b>, <b>Brake</b>, <b>Speed</b>, <b>CoastSetSw</b>, and <b>AccelResSw</b>.</p><p>11. With the (9) signals selected, click on the arrow next to the Siimulation Data Inspector button and select <b>Log Selected Signals</b>. (see picture below).</p><p>This will make small "Visualize Signal" icons appear on the signal lines, indicating that these signals will be streamed to the "Simulation Data Inspector".</p><p><img vspace="5" hspace="5" src="Step_04_SelectSignals.png" alt=""> </p><p>You should now have a model that looks like this:</p><p><img vspace="5" hspace="5" src="Step_04_HarnessReady4Logging.png" alt=""> </p><p>We can now run the simulation and inspect the results.</p><p><i>Note 1:</i> The steps above can be scripted.</p><p><i>Note 2:</i> If you haven't been able to complete the setup above you can open a pre-configured model by <a href="matlab:loadFinalShortHarness_SLT;">clicking here</a>.</p><h2>Executing the Functional Tests and Analyzing the Results<a name="9"></a></h2><p>Simulation Data Inspector (SDI) allows the user to:</p><div><ul><li>View logged data</li><li>Visually compare different signals within the same simulation</li><li>Compare the same signals between different simulations</li><li>Compare different simulation runs, perfect for Software-In-the-Loop (SIL) and Processor-In-the-Loop (PIL) comparisons Generate HTML reports of comparison results</li></ul></div><p>To run all (6) test cases and view the results in the Simulation Data Inspector, do the following:</p><p>1. Clear any runs in the Simulation Data Inspector - <b><a href="matlab:Simulink.sdi.clear;">click here</a></b>.</p><p>2. Open the <b>Signal Builder</b> block.</p><p>3. Click on the <b>Run All</b> button. This will run all (6) simulations in sequence.</p><p><img vspace="5" hspace="5" src="Step_04_SB_RunAll.png" alt=""> </p><p>Once the first simulation is complete, the Simulation Data Inspector (SDI) will open, containing the logged data from the simulation. After each run has completed, its simulation data will be added to SDI. When all (6) runs have completed, try to do the following actions in SDI:</p><p>4. There is a run for each of the test case with the streamed output signals.  Note the "verify" signals that are created in the "Test Assessment" block that provide the pass/fail analysis.</p><p>5. Analyze the results in the <b>Simulation Data Inspector</b> for the last test case "Disengage with Brake".</p><p><img vspace="5" hspace="5" src="Step_04_SDI_SignalCompare1.png" alt=""> </p><p>If you needed to compare other signals, the <b>Simulation Data Inspector</b> has a built-in "Comparison" feature for signals.  To show this feature we will compare the <b>tspeed</b> signals.</p><p>6. Select the "Compare" tab in SDI and select the "Signals" option.</p><p>7. Right-click on the "Exp_tspeed" signal, select <b>Compare Signals</b>, <b>Set as Baseline</b></p><p>8. Right-click on the "tspeed" signal, select <b>Compare Signals</b>, <b>Set as Compare To</b></p><p><img vspace="5" hspace="5" src="Step_04_SDI_SignalCompare2a.png" alt=""> </p><p><img vspace="5" hspace="5" src="Step_04_SDI_SignalCompare2b.png" alt=""> </p><p>Using "verify" signals and the SDI "Comparison" feature are ways to compare the Test Unit outputs to the expected outputs. But isn't there a way to automate this verification process? <b>Yes, there is!</b> In a later section we will use the more of the <b>Simulink Test</b> features including automated execution, evaluation and reporting.</p><p>For more information on Simulation Data Inspector, please refer to the <a href="matlab:web([docroot%20'/simulink/ug/visual-inspection-of-signal-data.html'])">Help</a> documentation.</p><h2>Test Sequence Blocks for Inputs and Online Assessment<a name="10"></a></h2><p>In this section we will use a test harness based on test sequence blocks for creating test inputs along with determining assessments.</p><p><img vspace="5" hspace="5" src="Step_04_SLTestSequenceOverview.png" alt=""> </p><p>Using <b>Test Sequence</b> blocks will result in a a more "natural language" approach to test case creation.  To show this approach we create a test based on the "Disengage with Brake" test case. From the test plan below it is easy to interpret the intention of the "Disengage with Brake" test case:</p><p><img vspace="5" hspace="5" src="Step_04_DisengageWithBrake.png" alt=""> </p><p>To show how the test sequence blocks can be used, do the following:</p><p>1. Open a harness model where these blocks have already been incorporated - <b><a href="matlab:loadTestSeqShortHarness_SLT;">click here</a>.</b></p><p><img vspace="5" hspace="5" src="Step_04_TestSeqHarness.png" alt=""> </p><p>2. Open the "Test Sequence" input blocks to see how the test case inputs have been interpreted.</p><p><img vspace="5" hspace="5" src="Step_04_TestSeqInputs.png" alt=""> </p><p>3. Open the "Test Assessment" blocks to see how the test case assessments have been interpreted.</p><p><img vspace="5" hspace="5" src="Step_04_TestSeqAssessment.png" alt=""> </p><p>4. Open the <b>Model Explorer</b> to see how the "Active_Step" is signal is configured as an output from the "Test Sequence" inputs block to be used by the "Test Assessment" block.</p><p><img vspace="5" hspace="5" src="Step_04_ActiveStepCfg.png" alt=""> </p><p>The "Active_Step" is an enumeration signal that enumeration values for all the steps in input block.  The "Active_Step" signal is then used by the "Test Assessment" to control the flow through the corresponding evaluation states.</p><p>5. Run the test to show the test passes.  Open "SDI" to check the verify signals. Also configure "SDI" to "Overwrite Run".</p><p><img vspace="5" hspace="5" src="Step_04_TestSeqSDIPass.png" alt=""> </p><p>6. Modify the "Test Assessment" block to demonstrate a failure similar to the previous example by changing the <b>tspeed</b> expected value from (50) to (40).</p><p><img vspace="5" hspace="5" src="Step_04_TestSeqAssessmentFail.png" alt=""> </p><p>7. Run the test case again.  The "tspeed:BrakeDisengage" signal now shows a failure.</p><p><img vspace="5" hspace="5" src="Step_04_TestSeqSDIFail.png" alt=""> </p><h2>Simulink Test Manager Overview<a name="11"></a></h2><p><img vspace="5" hspace="5" src="Step_04_SLTestManagerOverview1.png" alt=""> </p><p><img vspace="5" hspace="5" src="Step_04_SLTestManagerOverview2.png" alt=""> </p><h2>Importing a Test Harness into the Test Manager<a name="12"></a></h2><p>We will now show how to automatically create test cases by importing an existing <b>Simulink Test Harness</b>.  To show how the test harness import feature can be used, do the following:</p><p>1. Open a version of the <b>CruiseControl.slx</b> model with only one test harness - <b><a href="matlab:loadSingleHarnessMdl;">click here</a></b>.</p><p>2. Open the <b>Test Manager</b> by selecting <b>Analysis/Test Manager...</b> from the harness model menu.</p><p>3. Create a "Test File from Model" by <b>New</b>, <b>Test File</b>, <b>Test File from Model</b>.</p><p><img vspace="5" hspace="5" src="Step_04_TestFileFromMdl.png" alt=""> </p><p>4. Select "Use Current Model".  Enter "testSim" for the file in "Location". And select "Test type" as "Simulation".</p><p><img vspace="5" hspace="5" src="Step_04_NewTestFileUI.png" alt=""> </p><p>The test cases are automatically created for each Signal Builder test case in the "CruiseControl_Harness_ShortFinal" harness.</p><p>5. To run all the test cases, select the "CruiseControl_Harness_ShortFinal" test suite and press "Run".</p><p><img vspace="5" hspace="5" src="Step_04_RunSimTestFile.png" alt=""> </p><p>The test results show all test cases passing.  The data may be analyzed with embedded <b>Simulation Data Inspector</b>.</p><p><img vspace="5" hspace="5" src="Step_04_SimTestPass.png" alt=""> </p><p>6.In the Signal Builder block of the harness, change the expected tspeed for the "DisengageWithBrake" to (40) at time (1) to fail the test.</p><p><img vspace="5" hspace="5" src="Step_04_SigBldrSimTestFail.png" alt=""> </p><p>7. Re-run the "DisengageWithBrake" to see the failure displayed in the results.</p><p><img vspace="5" hspace="5" src="Step_04_SimTestFail.png" alt=""> </p><p>Notice the failure indicated in the results.  Also the "assert" block is terminating the simulation.  When this happens there are no simulation results to analyze.</p><h2>Introduction to the Model Coverage Concept<a name="13"></a></h2><p>In this section we will focus on measuring structural model coverage, a measurement of how much of the model has been exercised by your test cases.  Coverage is an important aspect of Verification and Validation. It can help you in several different ways:</p><div><ul><li>Determine dead logic "branches".</li><li>Determine if sufficient test vectors have been created</li><li>Determine if the existing requirements are sufficient</li></ul></div><p>The types of structural model coverage that are supported are:</p><div><ul><li>Decision</li><li>Condition</li><li>Modified Condition/Decision Coverage (MC/DC)</li></ul></div><p>An explanation of the above coverage metrics is shown for a simple model in the picture below:</p><p><img vspace="5" hspace="5" src="Step_04_ModelCoverageIntro.png" alt=""> </p><p>Please note that Simulink Verification and Validation tool offers more model coverage analysis capability than the ones listed above.  It can also collect coverage on:</p><div><ul><li>Signal Range</li><li>Lookup Table</li><li>Signal Size</li><li>Custom objectives using Simulink Design Verifier blocks</li></ul></div><h2>Collecting Model Coverage From Functional Test Cases<a name="14"></a></h2><p>Besides verifying that the unit under test behaves as expected w.r.t. functional requirements, it's also important to make sure that the test vectors have exercised the model to a high degree, i.e. that we have a high model coverage. For this example we will use the same model but with a more complete set of input test vectors based on the requirements. With this model we would expect the test cases would result in high model coverage beyond the "short" example.  To enable the "Test Manager" to measure the model coverage while executing a more complete set of test cases, do the following:</p><p>1. Open the <b>CruiseControl.slx</b> model with the high coverage test harness - <b><a href="matlab:loadCoverageHarnessMdl_SLT;">click here</a></b>.</p><p>Next we will manually create a test case in the <b>Test Manager</b> and use the "Iterations" feature.  This feature provides the ability to run all the sets of inputs in "Signal Builder" from a single <b>Test Manager</b> text case.</p><p>2. Open the <b>Test Manager</b>, navigate to the "TESTS" tab and select <b>New</b>, <b>Test File</b>, <b>Blank Test File</b> and enter "testCoverage" for the test file name.</p><p>3. Navigate to "New Test Case 1". Right-click to delete the default test case.</p><p>4. At the test suite level, "New Test Suite 1", right-click to create a new "Simulation Test".</p><p>5. For the "Model", select the "Use Current Model Icon" to use the "CruiseControl" model.</p><p>6. For the "Harness", select "CruiseControl_Harness_SB" from the dropdown.  This harness contains (14) test cases in the "Signal Builder" block.</p><p>7. In the "INPUTS" section, check "Signal Builder Group" and select "Refresh signal builder group list, performs update diagram".</p><p><img vspace="5" hspace="5" src="Step_04_CovTestCaseSetup.png" alt=""> </p><p>8.  Next we will configure the "Test Manager" to collect model coverage during the test execution.  Navigate to the test file level and enable coverage for referenced models select "Decision", "Condition" and "MC/DC" for the coverage metrics.</p><p><img vspace="5" hspace="5" src="Step_04_CoverageConfigSLT.png" alt=""> </p><p>We will now measure coverage for all referenced models (in our case the Test Unit - <b>CruiseControl</b>).</p><p>9. In the "ITERATIONS" section, select "Auto Generate" in "TABLE ITERATIONS" subsection.</p><p>10. In the "Iterations Templates" dialog, select "Signal Builder Group".</p><p><img vspace="5" hspace="5" src="Step_04_IterTemplateSetup.png" alt=""> </p><p>11. Highlight "New Test Case 1", and select the "Run" icon on the toolstrip as before.</p><p>After all (14) runs have been completed, the test results will be diplayed in the "Test Browser" but this time the results will include the model coverage metrics.  Notice all test cases pass with good but not complete coverage.  In the next section we will analyze the coverage metrics to help us understand what is missing.</p><p><img vspace="5" hspace="5" src="Step_04_TestResultsWithCovSLT.png" alt=""> </p><h2>Using the Model Coverage Results<a name="15"></a></h2><p>The Model Coverage analysis contains detailed information about what parts of the model are uncovered by the functional test cases.  The user can use this information to either:</p><div><ul><li><b>Develop more test vectors for the missing coverage.</b></li><li><b>Check to see if there are missing requirements.</b></li><li><b>Identify implementation design issues.</b></li></ul></div><p>The 92% overall decision coverage as shown in the previous section is relatively high for a first attempt.  The coverage goal may be as low 80% for a non-safety related applications but often we will attempt to get 100%, particularly if it is safety related.</p><p>We can look at a coverage report or show the results directly on the <b>Cruise Control</b> model to perform a detailed analysis.  To open the coverage report, you can select the "arrow" under the report column. But we will first show the coverage results on the model, by selecting "CruiseControl" under "Analyzed Model" column.</p><p><img vspace="5" hspace="5" src="Step_04_CovResultsLaunch.png" alt=""> </p><p>This provides a quick overview of what and how much coverage is missing. By selecting the transitions in the model we can see in detail what conditions are not covered by our test cases. The coverage results detail shown below provides insight into the completeness of our test cases.  We can also open a detailed report by clicking on the transition in the results detail window.</p><p><img vspace="5" hspace="5" src="Step_04_CovColors2Rpt.png" alt=""> </p><p><img vspace="5" hspace="5" src="Step_04_CovReportDetail.png" alt=""> </p><p>Reviewing the report shows the exit transition is never occurring for the vehicle speed limit check.  Likely this is due to either a missing requirement or test case.  When we check the requirements we realize that we need to add a few test cases to more completely cover this functional requirement.</p><p>The model coloring also shows that we are never exiting from a "hold" button input for the <b>AccelResSw</b> or increase speed button.  Based on this we found that we did not have complete requirements with regard to the "hold" function.</p><p><img vspace="5" hspace="5" src="Step_04_CovColors.png" alt=""> </p><p>We created (5) new test cases based on examining the model coverage results.</p><p>1.  These test cases have been added to a new "Signal Builder" harness called "CruiseControl_SB_Full".  Return to the <b>Test Manager</b>. Select this test harness in the test case configuration.</p><p>2. As before, refresh the "Signal Builder Group" to bring in the additional test cases.</p><p>3. Select and delete all iterations.</p><p>4. Select "Auto Generate" new iterations from the "Signal Builder Group". There should now be (19) iterations.</p><p>5. Highlight "New Test Case 1", and select the "Run" icon on the toolstrip as before.</p><p>In the <b>Test Manager</b>, the results show the last test case did not pass.</p><p><img vspace="5" hspace="5" src="Step_04_CovAssertErrorSLT.png" alt=""> </p><p>The "Test Assessment" block in the harness is that the implementation outputs <b>engage</b> and <b>tspeed</b> match the expected results.  Now that we have a greater number of test cases to cover more of the implementation, we find a design issue.  We can also look at the coverage results on the model that may help us locate the source of the design issue.</p><p><img vspace="5" hspace="5" src="Step_04_CovDesignIssue.png" alt=""> </p><p>We need to change the comparison operator for the (2) exit conditions from "&gt;" to "&gt;=" and "&lt;" to "&lt;=".</p><p>6.  Fix the issue for both exit conditions, or load the fixed version of the <b>CruiseControl</b> - <b><a href="matlab:loadCoverageHarnessMdlFix_SLT;">click here</a></b></p><p>7. Highlight "New Test Case 1", and select the "Run" icon on the toolstrip as before.</p><p><img vspace="5" hspace="5" src="Step_04_FullCovReport.png" alt=""> </p><p><img vspace="5" hspace="5" src="Step_04_FullCovColors.png" alt=""> </p><p>With the design fix and the additional test cases we now have 100% coverage and no assertion fail messages to the command window.</p><p>This shows a typical workflow where we iterated by analyzing the coverage results, add new functional est cases and eventually get to 100% coverage. Also realize that we know 100% coverage is possible because we fixed the error based on the order of integer calculations using Design Error Detection from previous section.</p><p>But, what if our logic is very complicated, and even after several iterations, we were only able to get to, say, 95% coverage? Is there anything we can do to speed up this iterative process? Yes, with Simulink Design Verifier, the user can also ask the tool to ignore the coverage already achieved by the functional test vectors and generate the missing test cases to achieve 100% coverage. The user can then e.g. use these test cases as "hints" to reverse engineer functional tests from them. This will be covered in the step <b>Test Generation</b>.</p><h2>Summary<a name="16"></a></h2><p>In this method we have shown a function verfication workflow:</p><div><ol><li>Creating an "internal" <b>test harness</b> within the implementation model</li><li>Importing test cases from a spreadsheet into the model</li><li>Adding a subsystem to do automatically check the outputs</li><li>Analyzing the results with the built-in Simulation Data Inspector (SDI)</li><li>Using <b>test sequence</b> blocks for creating a "natural language" test case</li><li>Automating the execution of test case with the <b>test Manager</b></li><li>Measuring the completeness of the test cases with model coverage</li><li>Using coverage and output comparisons to isolate and debug issues</li></ol></div><p>We were again able to find and fix these issues early in our development process, increasing confidence in our design.  We will continue to answer more of the questions in the next steps with our structured and formal testing framework for securing the quality, robustness and safety of our cruise controller.</p><p><img vspace="5" hspace="5" src="Step_04_CruiseControl_Summary.png" alt=""> </p><p>When you are finished, close all models and files - <b><a href="matlab:bdclose('all');">click here</a></b>.</p><p>Go to <b>Step 5: Test Case Generation</b> - <b><a href="Step_05.html">click here</a></b>.</p><p class="footer"><br><a href="http://www.mathworks.com/products/matlab/">Published with MATLAB&reg; R2019a</a><br></p></div><!--
##### SOURCE BEGIN #####
%% Step 4: Testing By Simulation
%
%% Introduction
%
% <<Step_04_CruiseControl_WhatNow.png>>
%
% The most common verification method is to actually execute the algorithm
% or program, i.e. so called dynamic testing. In Simulink this is done using
% simulation by simply pressing *PLAY*, or what we covered in the
% first step of the workshop, *Ad-hoc Testing*.  This step takes a more 
% formalized, structured approach to dynamic testing, goes beyond ad-hoc
% testing to answer the next questions:
% 
% * Does the implementation pass all functional requirements?
% * Do the tests completely exercise the implementation?
%
% Dynamic testing involves creating a set of test vectors based on the
% functional requirements, and ensuring that the algorithm output meets the 
% expected output. However, determining that the algorithm behaves as
% expected is only part of the overall test objectives. Another vital part
% is to get information about how much of the algorithm has been covered by
% the test cases. In Simulink, you can do this by enabling model coverage
% measurements during simulation. Model coverage is able to tell us whether
% or not we have achieved a "minimal" amount of testing. For example, if we
% have not reached 100% decision coverage, we have not exercised or tested
% 100% of our model. 
%
% In this section, we will see how we can quickly build up a test
% environment by creating a test harness model, capturing model coverage
% information and verify that our test results meet our expected outputs.
% We will use the test harness to debug the model when we fail the
% functional testing.  Model coverage results will also help the debugging 
% effort but more fundamentally it will provide an additional check that 
% the implementation matches the requirements beyond the previous 
% traceability discussion.
%  
% <<Step_04_SimTestWorklow.png>>
%
%% Verification and Validation Tools Used
% * Simulation Data Inspector (part of Simulink)
% * Simulink Verification and Validation
% * Simulink Test
%
%% Simulink Test Overview
%
% In this step we will use the features of *Simulink Test* to perform the
% dynamic testing of our cruise controller.  *Simulink Test* supports 
% interactive testing of the model with creation and management of *test
% harnesses*.  To create input vectors for use in the harness we will mostly
% use Signal Builder and a brief example of using a *test sequence* block 
% from *Simulink Test*.  For evaluation we will introduce the use of a 
% another instance of a *test sequence* block.  And lastly we will use the 
% *test manager* of *Simulink Test* to automate the execution, evaluation 
% and reporting of our results.
%
% <<Step_04_SLTestOverview.png>>
%
%% Test Harness Introduction
%
% Simulink Verification and Validation contains several functions that help
% the user create test harness models to facilitate the dynamic functional
% testing.  The test harness provides an environment to:
%
% * Import test cases
% * Link to requirements
% * Execute the tests
% * Evaluate the results
% * Debug the failed tests
% * Measure test completeness
%
% In the past, to create the test harness we would have used the 
% *slvnvmakeharness* function provided in the *Simulink Verification and 
% Validation* toolbox.  An identical function *sldvmakeharness* is provided
% with the *Simulink Design Verifier* toolbox.  But we will use the new 
% *test harness* creation feature provided by *Simulink Test*.
%
% <<Step_04_SLTestHarnessOverview.png>>
%
%% Creating a Test Harness at the Subsystem Level
%
% One of the features supporting algorithm development and testing is the
% ability to create a test harness at the subystem level.  In the past the
% algorithm developer would need to extract the subystem, manually
% create the harness and coordinate any changes between the main model and
% the harnessed subsystem.  With *Simulink Test* these steps are automated 
% and managed.
%
% Based on field testing of the Cruise Control, an issue was found by
% pressing both the "CoastSetSw" and the "AccelResSw" simultaneously.  The
% Cruise Control target speed was increased but this was not specified in
% the requirements, in fact it was missing. A new design was created to
% ignore the simultaneous input by implementing a subystem to condition the
% two inputs to be rejected.
%
% To examine the design and create the subsystem harness, do the following:
%
% 1.  Open "CruiseControl.slx" with the subsystem redesign - 
% *<matlab:C0_FuncTest_Sub; click here>*. 
%
% <<Step_04_SLTestSubReDesignMain.png>>
%
% 2.  Open the "RejectDoublePress" subsystem to examine the design.
%
% <<Step_04_SLTestSubReDesign.png>>
%
% 3.  Navigate to the top model.  Right-click on the "RejectDoublePress" 
% subsystem and select *
%
% <<Step_04_SLTestSubHarnessMenu.png>>
%
% 4.  Enter "RejectDoublePress_Wkshp_Harness" as the *Name* and select
% *Constant* as the source to create a harness to test the design 
% interactively.
%
% <<Step_04_SLTestSubHarnessDlg.png>>
%
% Once the harness has opened we can see the subsystem has been extracted 
% from the main model.  To complete the harness we can add "dashboard" 
% blocks to control the inputs and monitor the outputs.  The next step will
% detail the creation of a test harness. For this section, we will focus on 
% the subsystem testing workflow with an existing interactive test harness.
%
% <<Step_04_SLTestSubHarnessNew.png>>
%
% 5.  Close the harness by closing the window, and open the existing
% "RejectDoublePress_Adhoc_Harness".
%
% <<Step_04_SLTestSubHarnessOpen.png>>
%
% 6.  Run the test harness to see if it behaves as intended.
% 
% <<Step_04_SLTestSubHarnessResult.png>>
%
% Based on the testing, the design is behaving exactly opposite to what was
% intended.  It is rejecting single inputs and letting simulataneous input
% pass through to "ComputeTargeSpeed".
%
% 7.  To fix the design, modify the "AND" block to a "NAND" block.  Check
% the modified design for intended behavior.
%
% <<Step_04_SLTestSubHarnessFix.png>>
%
% 8.  Close the harness and examine the contents of the main model.  The
% subystem contents have been updated based on the work done in the
% harness.
%
% <<Step_04_SLTestHarnessFixInMain.png>>
%
%% Creating a Test Harness at the Model Level
%
% Now we will create a test harness at the model level.  To create the test 
% harness model, do the following:
% 
% 1.  Open *CruiseControl.slx* - *<matlab:loadFuncTestMdl; click here>*. 
%
% Examine the state chart to see the Cruise Control logic includes the bug
% fixes from our previous work:
%
% * Exit condition fix based on work done in *Step 0: 
% Ad-hoc Testing*
% * Calculation order and calibration lower limit fix based on work done in
% *Step 3:  Detecting Design Errors*
%
% 2.  We will create a harness with a *Signal Builder* block for the inputs and
% a *test sequence* block for the assessment.  To begin the creation of the
% test harness, select *Analysis*, *Test Harness*, 
% *Create Test Harness (Cruise Control) ...*.  If the menu selection for 
% *Create Test Harness (ComputeTargetSpeed)...* appears, you need to 
% de-select the *ComputeTargetSpeed* subsystem, otherwise you will be 
% creating a harness for the subsystem instead of the model.
% 
% <<Step_04_SLTestHarnessMenu.png>>
%
% 3.  In the "Create Test Harness" user interface, enter the name
% "CruiseControl_Harness_Short".  
%
% 4.  Select "Signal Builder" as the source block and check "Add separate
% assessment block". Select "OK" when
% completed.
%
% <<Step_04_SLTestHarnessUI.png>>
%
% The harness model consists of (6) components:
%
% # Source: in this case a Signal Builder block
% # Input Conversion Subsystem: Contains datatype and rate transition blocks.
% Buses and Vectors are also handled within this subsystem (read-only) 
% # Unit Under Test:  Model reference block containing Cruise Control model
% # Output Conversion Subsystem:  Contains datatype and rate transition blocks.
% Buses and Vectors are also handled within this subsystem (read-only)
% # Sink:  in this case the Outport blocks
% # Test Assessment Block:  Contains statements to compare the actual 
% outputs to the expected outputs
%
% <<Step_04_SLTestHarness.png>>
%
%% Importing Test Vectors 
%
% Based on our algorithm requirements, we have created a test plan
% document from which we have derived test vectors in an Excel file. This
% is a common way to define test cases in the industry. Next, we are going
% to populate the Signal Builder block with test vectors, including
% expected outputs, from the Excel file.  
%
% Do the following:
%
% 1.  Open the test plan document –
% *<matlab:winopen('cruise_control_testplan_short.docx') click here>*.
%
% <<Step_04_TestPlan.png>>
%
% 2.  Review the 6 test case descriptions.  This is only addressing a
% subset of the requirements for now.
%
% From the test plan, test cases were created by interpreting the test plan
% into a set time based test vectors and expected results.  The test cases
% were entered into a spreadsheet. Each test case is in its own worksheet 
% and the top labels are the same for all test cases. In a later section of
% this step we will use a larger set of test cases but for this introduction 
% the smaller set will be more efficient.
%
% 3.  Open the Excel file that contains the test vectors - 
% *<matlab:winopen('CruiseControlTests_short.xlsx') click here>*. For each 
% of the 6 test cases described in the test plan document, there is a
% corresponding Excel sheet.  
%
% <<Step_04_TestCases.png>>
%
% 4.  Open the *Signal Builder* block in the test harness and select 
% *File*, and *Import from File*.
%
% <<Step_04_SB_ImportMenu.png>>
%
% 5. Under *Select file to import*, click *Browse*, navigate to the *Test*
% directory, select the *CruiseControlTests_short.xlsx* file.  *Import 
% File* dialog will now have the *Files to Import* textbox populated.
%
% <<Step_04_SB_Import_FileSelect.png>>
%
% 6. After the data has been imported, check the *Select All*, *Replace
% existing dataset*, and *Confirm Selection options*, and then *Apply*.
%
% <<Step_04_SB_ImportUI.png>>
%
% 7. Select the *No, import without saving* option. The Signal Builder
% block should now have all the Excel data.
%
% <<Step_04_SB_ImportPopup.png>>
%
% <<Step_04_SB_TestCasesUI.png>>
%
%% Completing the Test Harness Model
%
% Please note that during the import, the signal lines between the Signal
% Builder block and the Test Unit have been disconnected. We now need to
% setup the harness model for easily logging and inspecting of the
% simulation results. Please do the following:
%
% 1.  If necessary, reconnect lines from "Signal Builder" to the 
% "Input Conversion Subsystem" block.
%
% 2.  For the expected output signals:  Add *Data Type Conversion* blocks,
% set the data type ("boolean" for *engaged* and "uint8" for *tspeed*).
%
% 3.  Label the signals to be *Exp_engaged* and *Exp_tspeed*.  Terminate 
% expected signals with *Goto* blocks with tag names matching the signal names. 
%
% <<Step_04_HarnessExpOutputs.png>>
%
% 4.  Open the "Test Assessment" block and remove all inputs except
% *engaged* and *tspeed*.  
%
% <<Step_04_TestAssessDeleteInputs.png>>
%
% 5.  Add inputs for *Exp_engaged* and *Exp_tspeed*.
%
% <<Step_04_TestAssessAddInputs.png>>
%
% 6.  Uncheck the "When decomposition" to create a simple step type.
%
% <<Step_04_TestAssessRemoveWhen.png>>
%
% 7.  Remove all substeps.  
%
% <<Step_04_TestAssessRemoveStep.png>>
%
% 8.  Add "verify" statements. 
%
% <<Step_04_TestAssessment.png>>
%
% 9.  Remove all unconnected lines and *From* blocks to the "Test
% Assessment Block".  For the *Exp_engaged* and *Exp_tspeed* signals:  
% Create *From* blocks and connect to "Test Assessment" block.
%
% You should now have a harness model that looks like the picture below:
%
% <<Step_04_HarnessConnected.png>>
%
% To log the signals for easy inspection, please do the following:
%
% 10.  Select the output signals: *engaged*, *tspeed*,
% *Exp_engaged*, and *Exp_tspeed*.  And select the input signals:  
% *CruiseOnOff*, *Brake*, *Speed*, *CoastSetSw*, and *AccelResSw*.
%
% 11. With the (9) signals selected, click on the arrow next to the 
% Siimulation Data Inspector button and select *Log Selected Signals*.
% (see picture below).
%
% This will make small "Visualize Signal" icons appear on the signal lines, 
% indicating that these signals will be streamed to the "Simulation Data Inspector".
%
% <<Step_04_SelectSignals.png>>
%
% You should now have a model that looks like this:
%
% <<Step_04_HarnessReady4Logging.png>>
%
% We can now run the simulation and inspect the results.
%
% _Note 1:_ The steps above can be scripted.
%
% _Note 2:_ If you haven't been able to complete the setup above you can open
% a pre-configured model by
% <matlab:loadFinalShortHarness_SLT; clicking here>.
%
%% Executing the Functional Tests and Analyzing the Results  
%
% Simulation Data Inspector (SDI) allows the user to:
% 
% * View logged data
% * Visually compare different signals within the same simulation
% * Compare the same signals between different simulations
% * Compare different simulation runs, perfect for Software-In-the-Loop
% (SIL) and Processor-In-the-Loop (PIL) comparisons
% Generate HTML reports of comparison results
%
% To run all (6) test cases and view the results in the Simulation Data
% Inspector, do the following: 
%
% 1. Clear any runs in the Simulation Data Inspector - *<matlab:Simulink.sdi.clear; click here>*.
%
% 2. Open the *Signal Builder* block.
%
% 3. Click on the *Run All* button. This will run all (6) simulations in
% sequence.
% 
% <<Step_04_SB_RunAll.png>>
%
% Once the first simulation is complete, the Simulation Data Inspector
% (SDI) will open, containing the logged data from the simulation. After
% each run has completed, its simulation data will be added to SDI. When
% all (6) runs have completed, try to do the following actions in SDI:
%
% 4. There is a run for each of the test case with the streamed output 
% signals.  Note the "verify" signals that are created
% in the "Test Assessment" block that provide the pass/fail analysis.   
%
% 5. Analyze the results in the *Simulation Data Inspector* for
% the last test case "Disengage with Brake".  
%
% <<Step_04_SDI_SignalCompare1.png>>
%
% If you needed to compare other signals, the *Simulation Data Inspector* 
% has a built-in "Comparison" feature for signals.  To show this feature we
% will compare the *tspeed* signals.
%
% 6. Select the "Compare" tab in SDI and select the "Signals" option.
%
% 7. Right-click on the "Exp_tspeed" signal, select *Compare Signals*, 
% *Set as Baseline*
%
% 8. Right-click on the "tspeed" signal, select *Compare Signals*, *Set as
% Compare To*
%
% <<Step_04_SDI_SignalCompare2a.png>>
%
% <<Step_04_SDI_SignalCompare2b.png>>
%
% Using "verify" signals and the SDI "Comparison" feature are ways to 
% compare the Test Unit outputs to the expected outputs. But isn't there a 
% way to automate this verification process? *Yes, there is!* In a later 
% section we will use the more of the *Simulink Test* features including 
% automated execution, evaluation and reporting.
% 
% For more information on Simulation Data Inspector, please refer to the
% <matlab:web([docroot%20'/simulink/ug/visual-inspection-of-signal-data.html'])
% Help> documentation.
%
%% Test Sequence Blocks for Inputs and Online Assessment
%
% In this section we will use a test harness based on test sequence blocks
% for creating test inputs along with determining assessments.  
%
% <<Step_04_SLTestSequenceOverview.png>>
%
% Using *Test Sequence* blocks will result in a 
% a more "natural language" approach to test case creation.  To show this 
% approach we create a test based on the "Disengage with Brake" test case.
% From the test plan below it is easy to interpret the intention of the 
% "Disengage with Brake" test case:
%
% <<Step_04_DisengageWithBrake.png>>
%
% To show how the test sequence blocks can be used, do the following:
%
% 1. Open a harness model where these blocks have already been incorporated 
% - *<matlab:loadTestSeqShortHarness_SLT; click here>.*
% 
% <<Step_04_TestSeqHarness.png>>
%
% 2. Open the "Test Sequence" input blocks to see how the test case inputs 
% have been interpreted.
%
% <<Step_04_TestSeqInputs.png>>
%
% 3. Open the "Test Assessment" blocks to see how the test case assessments 
% have been interpreted.
%
% <<Step_04_TestSeqAssessment.png>>
%
% 4. Open the *Model Explorer* to see how the "Active_Step" is signal is
% configured as an output from the "Test Sequence" inputs block to be used
% by the "Test Assessment" block.  
%
% <<Step_04_ActiveStepCfg.png>>
%
% The "Active_Step" is an enumeration signal that enumeration values for 
% all the steps in input block.  The "Active_Step" signal is then used by
% the "Test Assessment" to control the flow through the corresponding
% evaluation states.
%
% 5. Run the test to show the test passes.  Open "SDI" to check the verify 
% signals. Also configure "SDI" to "Overwrite Run".
%
% <<Step_04_TestSeqSDIPass.png>>
%
% 6. Modify the "Test Assessment" block to demonstrate a failure similar to
% the previous example by changing the *tspeed* expected value from (50) to
% (40).
%
% <<Step_04_TestSeqAssessmentFail.png>>
%
% 7. Run the test case again.  The "tspeed:BrakeDisengage" signal now shows
% a failure.
%
% <<Step_04_TestSeqSDIFail.png>>
%
%% Simulink Test Manager Overview
%
% <<Step_04_SLTestManagerOverview1.png>>
%
% <<Step_04_SLTestManagerOverview2.png>>
%
%% Importing a Test Harness into the Test Manager
%
% We will now show how to automatically create test cases by importing an
% existing *Simulink Test Harness*.  To show how the test harness import 
% feature can be used, do the following:
%
% 1. Open a version of the *CruiseControl.slx* model with only one test 
% harness - *<matlab:loadSingleHarnessMdl; click here>*. 
%
% 2. Open the *Test Manager* by selecting *Analysis/Test Manager...* from
% the harness model menu.
%
% 3. Create a "Test File from Model" by *New*, *Test File*, *Test File from Model*. 
%
% <<Step_04_TestFileFromMdl.png>>
%
% 4. Select "Use Current Model".  Enter "testSim" for the file in
% "Location". And select "Test type" as "Simulation".
%
% <<Step_04_NewTestFileUI.png>>
%
% The test cases are automatically created for each Signal Builder test
% case in the "CruiseControl_Harness_ShortFinal" harness.
%
% 5. To run all the test cases, select the
% "CruiseControl_Harness_ShortFinal" test suite and press "Run".
%
% <<Step_04_RunSimTestFile.png>>
%
% The test results show all test cases passing.  The data may be analyzed
% with embedded *Simulation Data Inspector*.
%
% <<Step_04_SimTestPass.png>>
%
% 6.In the Signal Builder block of the harness, change the expected tspeed 
% for the "DisengageWithBrake" to (40) at time (1) to fail the test.
%
% <<Step_04_SigBldrSimTestFail.png>>
%
% 7. Re-run the "DisengageWithBrake" to see the failure displayed in the
% results.
%
% <<Step_04_SimTestFail.png>>
%
% Notice the failure indicated in the results.  Also the "assert" block is
% terminating the simulation.  When this happens there are no simulation
% results to analyze.  
%
%% Introduction to the Model Coverage Concept
%
% In this section we will focus on measuring structural model coverage, a 
% measurement of how much of the model has been exercised by your test 
% cases.  Coverage is an important aspect of Verification and Validation.
% It can help you in several different ways:
%
% * Determine dead logic "branches".
% * Determine if sufficient test vectors have been created
% * Determine if the existing requirements are sufficient
% 
% The types of structural model coverage that are supported are:
%
% * Decision
% * Condition
% * Modified Condition/Decision Coverage (MC/DC)
% 
% An explanation of the above coverage metrics is shown for a simple model
% in the picture below:
%
% <<Step_04_ModelCoverageIntro.png>>
%
% Please note that Simulink Verification and Validation tool offers more
% model coverage analysis capability than the ones listed above.  It can
% also collect coverage on:  
%
% * Signal Range
% * Lookup Table
% * Signal Size
% * Custom objectives using Simulink Design Verifier blocks
%
%% Collecting Model Coverage From Functional Test Cases
%
% Besides verifying that the unit under test behaves as expected w.r.t.
% functional requirements, it's also important to make sure that the test
% vectors have exercised the model to a high degree, i.e. that we have a
% high model coverage. For this example we will use the same model but with
% a more complete set of input test vectors based on the requirements.
% With this model we would expect the test cases would result in high model
% coverage beyond the "short" example.  To enable the "Test Manager" to 
% measure the model coverage while executing a more complete set of test 
% cases, do the following:
%
% 1. Open the *CruiseControl.slx* model with the high coverage test 
% harness - *<matlab:loadCoverageHarnessMdl_SLT; click here>*. 
%
% Next we will manually create a test case in the *Test Manager* and
% use the "Iterations" feature.  This feature provides the ability to run
% all the sets of inputs in "Signal Builder" from a single *Test Manager*
% text case.
% 
% 2. Open the *Test Manager*, navigate to the "TESTS" tab and select *New*,
% *Test File*, *Blank Test File* and enter "testCoverage" for the test file
% name.
%
% 3. Navigate to "New Test Case 1". Right-click to delete the default test case. 
%
% 4. At the test suite level, "New Test Suite 1", right-click to create a 
% new "Simulation Test".
%
% 5. For the "Model", select the "Use Current Model Icon" to use the 
% "CruiseControl" model. 
%
% 6. For the "Harness", select "CruiseControl_Harness_SB" from the
% dropdown.  This harness contains (14) test cases in the "Signal Builder"
% block.
%
% 7. In the "INPUTS" section, check "Signal Builder Group" and select "Refresh
% signal builder group list, performs update diagram".
%
% <<Step_04_CovTestCaseSetup.png>>
%
% 8.  Next we will configure the "Test Manager" to collect model coverage 
% during the test execution.  Navigate to the test file level and enable 
% coverage for referenced models select "Decision", "Condition" and "MC/DC"
% for the coverage metrics.
%
% <<Step_04_CoverageConfigSLT.png>>
%
% We will now measure coverage for all referenced models (in our case the
% Test Unit - *CruiseControl*). 
%
% 9. In the "ITERATIONS" section, select "Auto Generate" in "TABLE
% ITERATIONS" subsection.
%
% 10. In the "Iterations Templates" dialog, select "Signal Builder Group".
%
% <<Step_04_IterTemplateSetup.png>>
%
% 11. Highlight "New Test Case 1", and select the "Run" icon on the
% toolstrip as before.
%
% After all (14) runs have been completed, the test results will be diplayed
% in the "Test Browser" but this time the results will include the model
% coverage metrics.  Notice all test cases pass with good but not complete
% coverage.  In the next section we will analyze the coverage metrics to
% help us understand what is missing.
%
% <<Step_04_TestResultsWithCovSLT.png>>
%
%
%% Using the Model Coverage Results
%
% The Model Coverage analysis contains detailed information about what parts 
% of the model are uncovered by the functional test cases.  The user can 
% use this information to either: 
%
% * *Develop more test vectors for the missing coverage.*
% * *Check to see if there are missing requirements.* 
% * *Identify implementation design issues.*
% 
% The 92% overall decision coverage as shown in the previous section is 
% relatively high for a first attempt.  The coverage goal may be as low 80%
% for a non-safety related applications but often we will attempt to get 
% 100%, particularly if it is safety related. 
%
% We can look at a coverage report or show the results directly on the
% *Cruise Control* model to perform a detailed analysis.  To open the 
% coverage report, you can select the "arrow" under the report column. But we 
% will first show the coverage results on the model, by selecting 
% "CruiseControl" under "Analyzed Model" column.  
%
% <<Step_04_CovResultsLaunch.png>>
%
% This provides a quick overview of what and how much coverage is missing. 
% By selecting the transitions in the model we can see in detail what 
% conditions are not covered by our test cases. The coverage results detail
% shown below provides insight into the completeness of our test cases.  We
% can also open a detailed report by clicking on the transition in the 
% results detail window.
%
% <<Step_04_CovColors2Rpt.png>>
%
% <<Step_04_CovReportDetail.png>>
%
% Reviewing the report shows the exit transition is never occurring for the 
% vehicle speed limit check.  Likely this is due to either a missing 
% requirement or test case.  When we check the requirements we realize that 
% we need to add a few test cases to more completely cover this functional 
% requirement.
%
% The model coloring also shows that we are never exiting from a "hold" 
% button input for the *AccelResSw* or increase speed button.  Based on 
% this we found that we did not have complete requirements with regard to 
% the "hold" function.
% 
% <<Step_04_CovColors.png>>
%
% We created (5) new test cases based on examining the model coverage
% results.
%
% 1.  These test cases have been added to a new "Signal Builder" harness 
% called "CruiseControl_SB_Full".  Return to the *Test Manager*. Select 
% this test harness in the test case configuration. 
%
% 2. As before, refresh the "Signal Builder Group" to bring in the 
% additional test cases.
%
% 3. Select and delete all iterations.
%
% 4. Select "Auto Generate" new iterations from the "Signal Builder Group".
% There should now be (19) iterations. 
%
% 5. Highlight "New Test Case 1", and select the "Run" icon on the
% toolstrip as before.
%
% In the *Test Manager*, the results show the last test case did not pass.
%
% <<Step_04_CovAssertErrorSLT.png>>
%
% The "Test Assessment" block in the harness is that the implementation
% outputs *engage* and *tspeed* match the expected results.  Now that we 
% have a greater number of test cases to cover more of the implementation, 
% we find a design issue.  We can also look at the coverage results on the 
% model that may help us locate the source of the design issue.
%
% <<Step_04_CovDesignIssue.png>>
%
% We need to change the comparison operator for the (2) exit conditions from 
% ">" to ">=" and "<" to "<=".
%
% 6.  Fix the issue for both exit conditions, or load the fixed version of
% the *CruiseControl*
% - *<matlab:loadCoverageHarnessMdlFix_SLT; click here>*
%
% 7. Highlight "New Test Case 1", and select the "Run" icon on the
% toolstrip as before.
%
% <<Step_04_FullCovReport.png>>
%
% <<Step_04_FullCovColors.png>>
%
% With the design fix and the additional test cases we now have 100%
% coverage and no assertion fail messages to the command window.
%
% This shows a typical workflow where we iterated by analyzing the
% coverage results, add new functional est cases and eventually get to 100% coverage.  
% Also realize that we know 100% coverage is possible because we fixed the error
% based on the order of integer calculations using Design Error Detection from
% previous section.
%
% But, what if our logic is very complicated, and even after several 
% iterations, we were only able to get to, say, 95% coverage?
% Is there anything we can do to speed up this iterative process?     
% Yes, with Simulink Design Verifier, the user can also ask the tool to
% ignore the coverage already achieved by the functional test vectors and 
% generate the missing test cases to achieve 100% coverage. The user can
% then e.g. use these test cases as "hints" to reverse engineer functional
% tests from them. This will be covered in the step *Test Generation*.
%
%% Summary
%
% In this method we have shown a function verfication workflow:
%
% # Creating an "internal" *test harness* within the implementation model
% # Importing test cases from a spreadsheet into the model
% # Adding a subsystem to do automatically check the outputs
% # Analyzing the results with the built-in Simulation Data Inspector (SDI)
% # Using *test sequence* blocks for creating a "natural language" test case
% # Automating the execution of test case with the *test Manager*
% # Measuring the completeness of the test cases with model coverage
% # Using coverage and output comparisons to isolate and debug issues
%
% We were again able to find and fix these issues early in our 
% development process, increasing confidence in our design.  We will 
% continue to answer more of the questions in the next steps with our 
% structured and formal testing framework for securing the quality, 
% robustness and safety of our cruise controller.    
%
% <<Step_04_CruiseControl_Summary.png>>
%
% When you are finished, close all models and files - 
% *<matlab:bdclose('all'); click here>*.
%
% Go to *Step 5: Test Case Generation* - *<Step_05.html click here>*.
%
##### SOURCE END #####
--></body></html>