
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>Step 5: Test Case Generation</title><meta name="generator" content="MATLAB 9.5"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2018-10-28"><meta name="DC.source" content="Step_05.m"><style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:100%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:14px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:2.0em; color:#000077; line-height:150%; font-weight:bold; text-align:center }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.6em; color:#444444; font-weight:bold; font-style:italic; text-align:left; vertical-align:bottom; line-height:200%; border-top:2px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.1em; color:#000; font-weight:bold; line-height:140%; }

a { color:#555555; font-style:italic; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img, h1 img, h2 img { margin-bottom:0px;} 

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }

pre, code { font-size:12px; }
tt { font-size: 1.2em; }
pre { margin:0px 0px 20px; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }
pre.error { color:red; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }

.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; }
.footer a { color:#878787; }
.footer a:hover { color:#878787; text-decoration:underline; }
.footer a:visited { color:#878787; }

table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }





  </style></head><body><div class="content"><h1>Step 5: Test Case Generation</h1><!--introduction--><!--/introduction--><h2>Contents</h2><div><ul><li><a href="#1">Introduction</a></li><li><a href="#2">Verification and Validation Tools Used</a></li><li><a href="#3">Collect Initial Model Coverage with Simulink Test</a></li><li><a href="#4">Generate Test Cases for Missing Model Coverage</a></li><li><a href="#5">Run Full Coverage Test Cases in Simulink Test</a></li><li><a href="#6">Generate a Test from a Requirements Model</a></li><li><a href="#7">Generate Tests from a Requirements Assessment</a></li><li><a href="#8">Summary</a></li></ul></div><h2>Introduction<a name="1"></a></h2><p>In this section, we are going to collect coverage on our cruise control module using the test harness model with the initial set of input test vectors.  The coverage analysis from running these test vector provided high but partial model coverage.  As before we are going to run these initial test cases in <b>Simulink Test</b> but this time we will ask <b>Simulink Design Verifier</b> to generate the test cases to cover the missing coverage objectives.  The test generation tool can ignore the already satisfied objectives and only create test cases that when combined with the inital test vectors provide complete model coverage. This is typically called a "top-it-off" test generation workflow.  In this step we will see how the integration of <b>Simulink Test</b> and <b>Simulink Design Verifier</b> supports this workflow.</p><p>This step is a prerequisite to the next step <b>Code Verification</b> where we will perform "equivalence testing".  In order to verify equivalent behaviors between the model and the generated code, the comparison analysis requires 100% model coverage, input test vectors. In addition we will verify that the code generation did not create unintended functionality by comparing model and code coverage.  With 100% model coverage, sections of the code with incomplete coverage may indicate possible unintended functionality.</p><p>In our previous step we were able to analyze the coverage results on the model, identify missing requirements and incomplete test cases to achieve 100% coverage.  On a more complex model or within a tight development schedule, a "top-it-off" workflow is often used to quickly create a set of test vectors to support equivalence testing.  A side benefit to the test generation is to identify dead logic when the test generation procedure reports certain coverage objectives as "unsatisfiable" although this is now typically done with the <b>Design Error Detection</b> feature of <b>Simulink Design Verifier</b>.</p><p>In addition we will look at generating tests based on requirements.  For these tests the functional behavior of the requirements is modeled. <b>Simulink Design Verifier</b> will generate a test based on the conditions of the functional behavior model.  The technique can be applied to critical behaviors or to completely demonstrate behavior compliance to the requirements.  Generally the additional effort to create the behavior models is best applied to the critical or safety related behaviors.</p><h2>Verification and Validation Tools Used<a name="2"></a></h2><div><ul><li>Simulink Test</li><li>Simulink Verification and Validation</li><li>Simulink Design Verifier</li></ul></div><h2>Collect Initial Model Coverage with Simulink Test<a name="3"></a></h2><p>As we said in the introduction we will be using our partial coverage test harness model with the (14) test cases from the previous step but with a "Cruise Control" model with all the previous bug fixes.  <b>Simulink Test</b> will be used to run the tests and collect the intial coverage.</p><p>Do the following:</p><p>1. Load the <b>Simulink Test</b>, <b>Test Manager</b> configuration file named "topItOff" - <b><a href="matlab:C5_TopItOffTestGen;">click here</a></b>.</p><p>2. Navigate to the test case "Partial Cov" and analyze the configuration.</p><p><img vspace="5" hspace="5" src="Step_05_TestMgrTopItOff.png" alt=""> </p><p>3. We will use the <b>Test Manager</b> to open the "Cruise Control" model with all the bug fixes including the logic issue we found in the last step. Open the "Cruise Control" model by clicking the "arrow" next to the <b>Model</b> field.</p><p>We will be generating test cases on a production version that is ready for code generation.  Typically test cases are generated on a production version to support equivalence testing to verify the behavior of the model and code match.  We will run the equivalence test in the next step <b>Component Code Verification</b>.  For now review the production version to confirm the model contains the bug fixes.</p><p><img vspace="5" hspace="5" src="Step_05_NoDesignIssue.png" alt=""> </p><p>4. Open the test harness "CruiseControl_Harness_SB" containing the partial coverage test vectors by clicking on the "arrow" next to the <b>Harness</b> field.</p><p><img vspace="5" hspace="5" src="step_05_PartialCovHarness.png" alt=""> </p><p>Confirm the test harness model contains the partial coverage test vectors.</p><p><img vspace="5" hspace="5" src="step_05_PartialCovSigBldr.png" alt=""> </p><p>The <b>Test Manager</b> configuration is very similar to what was configured manually in the previous step <b>Testing by Simulation</b>.  It has been configured for you so we can focus on test generation for coverage.  The details of how to configure the <b>Test Manager</b> can be found in the previous step <b>Testing by Simulation</b>.  Let's review the test case configuration.</p><p>5. Return to the <b>Test Manager</b> to check that the <b>Signal Builder</b> test cases have been configured to be used in the <b>Inputs</b> section.</p><p><img vspace="5" hspace="5" src="Step_05_InputsSetupInitCov.png" alt=""> </p><p>6. Examine the <b>Iterations</b> configuration to see that we will iterate across the (14) <b>Signal Builder</b> test cases.  Notice the <b>Scripted Iterations</b> method is being so select <b>Show Iterations</b> to confirm the (14) initial test cases will be executed.</p><p><img vspace="5" hspace="5" src="Step_05_ItersSetupInitCov.png" alt=""> </p><p>7. Check the file level <b>coverage settings</b> configuration to see that decision, condition and MCDC coverage has been enabled at the file level and the test case level.</p><p><img vspace="5" hspace="5" src="Step_05_TestMgrCovSettings.png" alt=""> </p><p>8. We are ready to execute the partial coverage, test vectors and collect the coverage results.  At the "Partial Cov" test case level, select <b>Run</b>.</p><p><img vspace="5" hspace="5" src="Step_05_RunInitCov.png" alt=""> </p><p>From the results, we can see that (14) test cases have been successfully executed and the model coverage has been measured.</p><p><img vspace="5" hspace="5" src="Step_05_PartialCovResults.png" alt=""> </p><p>This completes the initial model coverage collection task.  Next we will generate test vectors for complete coverage.</p><h2>Generate Test Cases for Missing Model Coverage<a name="4"></a></h2><p>The coverage results from the initial test vectors were:</p><div><ul><li>92% Decision Coverage</li><li>81% Condition Coverage</li><li>63% MCDC Coverage</li></ul></div><p><b>Simulink Design Verifier</b> will be used to create additional test cases that will provide the missing coverage.  When the generated test cases are executed along with the initial test cases the coverage analysis will show 100% model coverage.  Now let&#8217;s see how the integration of <b>Simulink Design Verifier</b> facilitates the generation of these "top-it-off" test cases.</p><p>From <b>Results Explorer</b> pane in the <b>Test Manager</b> we will begin the "top-it-off" test case generation.</p><p>1.  <b>Click</b> on <b>Add Tests for Missing Coverage</b></p><p><img vspace="5" hspace="5" src="Step_05_AddMissingTests.png" alt=""> </p><p>2.  Populate the <b>Add Tests for Missing Coverage</b> dialog as shown.</p><p><img vspace="5" hspace="5" src="Step_05_AddMissingTestsDlg.png" alt=""> </p><p>A dialog popups to show the status of the test case generation. <b>Simulink Design Verifier</b> is only generating test cases for the missing coverage by using the coverage results from the <b>Test Manager</b>.</p><p><img vspace="5" hspace="5" src="Step_05_DV_ProgressDlg.png" alt=""> </p><p>When the analysis has completed, <b>Simulink Design Verifier</b> has generated test cases for the (15) coverage objectives, and exports the test cases to the <b>Test Manager</b> in the new test case "New Test Case 1".</p><p><img vspace="5" hspace="5" src="Step_05_ExpTestCasesInTestMgr.png" alt=""> </p><p>3.  Cut and paste the exported test cases into the original test suite.</p><p>4.  Rename exported test to "Generated"</p><p><img vspace="5" hspace="5" src="Step_05_ExpTestCasesMovedRenamed.png" alt=""> </p><p>A new test case "Generated" has been created in the <b>Test Manager</b>.  In the next section we will look at the contents of the generated test cases and run all the test cases to demonstrate full coverage.</p><h2>Run Full Coverage Test Cases in Simulink Test<a name="5"></a></h2><p>In the <b>Test Manager</b> we have a new test case that was created during the export of the generated test cases to <b>Simulink Test</b>.  Let's examine the contents of the generated test case "Generated" and then run all the tests for the module:</p><p>1.  Navigate to the test case "Generated".  The test case is a <b>Baseline Test</b> type so the evaluation will be based on external files for the comparison.</p><p><img vspace="5" hspace="5" src="Step_05_TestCaseCfgHarness.png" alt=""> </p><p>2.  Looking at the <b>SYSTEM UNDER TEST</b> section, the <b>Model</b> is "CruiseControl" as expected but the <b>Harness</b> is the newly, generated "CruiseControl_Harness1".  The harness is a <b>Simulink Test</b> harness that is now contained in the model.</p><p>The test case is using the <b>Simulink Design Verifier</b> the (8) generated test cases along with the expected results.</p><p>3.  Expand the <b>INPUTS</b> section to see the <b>EXTERNAL INPUTS</b> section has been populated with test cases from the <b>Simulink Design Verifier</b> data file "CruiseControl_sldvdata.mat".</p><p><img vspace="5" hspace="5" src="Step_05_TestCaseCfgInputsBaseline.png" alt=""> </p><p>4. Expand the <b>BASELINE CRITERIA</b> section to see the test case has been populated with expected outputs from the test generation data file. These were included in the export as a result of configuring <b>Design Verifier</b> to <b>Include expected output values</b>.</p><p><img vspace="5" hspace="5" src="Step_05_TestGenCfgExpValues.png" alt=""> </p><p>These values are outputs that were measured by executing the generated test case inputs.  So these expected output test vectors may not be useful for checking the current model but will be useful for checking the generated code in the next step and for checking later versions of the model.</p><p>5. Look at the <b>ITERATIONS</b> section to see the test case will be iterated over the input data files and the expected output data files. Optionally check <b>Run test iterations in fast restart</b>.</p><p><img vspace="5" hspace="5" src="Step_05_TestGenCfgIterations.png" alt=""> </p><p>We will be executing the original, partial coverage test vectors along with the generated test vectors to produce complete coverage.  As a personal choice, the generated test cases were copied into the original test suite so all the test will be executed from a single suite. This way all the test cases for the "Cruise Control" model will be contained in a single test file.  But you may choose to have these in separate files depending on your process.</p><p>6.  Run the tests from the "topItOff/Test Suite" level to measure the expected full coverage.</p><p><img vspace="5" hspace="5" src="Step_05_RunReqAndGenTests.png" alt=""> </p><p>In the <b>Results and Artifacts</b> pane the test results show 100% coverage. Notice there are the original (14) requirements based test vectors along with the (8) coverage based generated test vectors.  Depending on the available development time, the generated test cases may be used as hints that can be used to reverse engineer functional test cases from them, associate them with a requirement and create expected outputs. Generating test cases for functional objectives is discussed in the nest section.  But for the purpose of equivalence testing we are ready for the next step <b>Component Verification</b> with these structural test cases.</p><p><img vspace="5" hspace="5" src="Step_05_TestMgrGenTestResults.png" alt=""> </p><h2>Generate a Test from a Requirements Model<a name="6"></a></h2><p>In this section we will demonstrate the use of the <b>Test Generation</b> feature for functional tests.  The behavior as described in the requirements is expressed as a model with two parts:  (1) an expected action or output behavior based on (2) when a condition or event occurs. The condition can include a logical sequence of current and past inputs, and often past outputs.  The expected action or output behavior model will also include current outputs.</p><p>For the first example we will return to the "Disengage With Brake" requirement and build a requirements based model for test generation.</p><p>Do the following:</p><p>1.  Open the <b>CruiseControl_TestGen_DisengageWithBrake</b> model &#8211; <b><a href="matlab:D0_TestGenDisengageWithBrake()">click here</a></b></p><p>2.  Navigate to the "Disengage with Brake" requirement in the editor.</p><p><img vspace="5" hspace="5" src="Step_05_FuncGenReq.png" alt=""> </p><p>Notice this requirement is derived from a higher level safety requirement, linked to a test case and implemented by the transition "[Brake||Speed&gt;maxtspeed||Speed&lt;mintspeed]".</p><p>3.  Select the implementation link to navigate to the model transition.</p><p><img vspace="5" hspace="5" src="Step_05_FuncGenSF.png" alt=""> </p><p>4.  Determine the requirements linked to this transition.</p><p>Notice that based on the implementation and another requirement, a low or high speed can also execute the transition.</p><p>5.  Navigate to the <b>CruiseControl_TestGen_DisengageWithBrake</b> model, and open the "Verification Subsystem".</p><p><img vspace="5" hspace="5" src="Step_05_FuncGenReqTO.png" alt=""> </p><p>The highlighted blocks is the part of the requirements model that will be used by <b>Design Verifier</b> to generate the test.  The test case will create the condition described in the requirement.  The "DisengageWithBrake_Test" is a <b>Test Objective</b> block that will instruct the tool to create a test case where its input is true for at least one time step.  Study the input signal to the test objective to understand how this represents the condition of the requirement.</p><p>The test case generation includes a block to constrain "Speed" signal to be between the limits of (20) and (90).  The generated test will not include a "Speed" signal that could cause a disengagement to cloud the applicability of the test case.</p><p><img vspace="5" hspace="5" src="Step_05_FuncGenReqTC.png" alt=""> </p><p>6.  Generate the test case by the menu selection <b>Analysis/Design Verifier/Generate Tests/Model</b></p><p><img vspace="5" hspace="5" src="Step_05_FuncGenResult.png" alt=""> </p><p>7.  Select <b>Export test cases to Simulink Test</b>.  Use the default settings to complete the export.</p><p><img vspace="5" hspace="5" src="Step_05_FuncGenSLTestExport.png" alt=""> </p><p>8.  Run the test in the <b>Test Manager</b> and analyze the results.</p><p><img vspace="5" hspace="5" src="Step_05_FuncGenSLTestSDI.png" alt=""> </p><p>Notice the <b>Test Manager</b> shows a pass with a "verify" signal.  How was the "verify signal" generated?</p><p><img vspace="5" hspace="5" src="Step_05_FuncGenReqVerify.png" alt=""> </p><p>The "verify" signal was generated with highlighted blocks in the "Verification Subsystem".  Looking at the "Implies" block:</p><div><ul><li>When the condition at input "A" is true</li><li>Then the action (expected output) at "B" is expected to be true</li><li>And the output result is true</li><li>When the condition at input "A" is false then the output result is "true" regardless of whether "B" is true or false</li></ul></div><p>The "Implies" block output is connected to an "assertion" block that creates the "verify" signal.  The "assertion" provided a test result assessment to the <b>Test Manager</b> based on the single generated test case.</p><p>Are there other test cases that could be generated based on the "test objective" but fail the "assertion"?  Meaning the implementation will not produce the expected output or action?  <b>Design Verifier</b> provides another function known as <b>Property Proving</b> to prove there are no test case inputs that can fail the "assertion".  If <b>Property Proving</b> can falsify the "assertion" or proof then it will create a test case for the user to debug the implementation.  In a later step we will go into more detail but for now we will perform the <b>Property Proving</b> to see if there is a test case that can show the implementation will not meet the requirement.</p><p>9.  Make the menu selection <b>Analysis/Design Verifier/Prove Property/Model</b> to perform the analysis.</p><p><img vspace="5" hspace="5" src="Step_05_FuncGenProveResult.png" alt=""> </p><p>On the model the "assertion" block is colored "green" to indicate the property proof was satisfied (not falsified).  In the later step we will show how property proving can be used to iteratively improve the robustness of the design.</p><p><img vspace="5" hspace="5" src="Step_05_FuncGenReqProve.png" alt=""> </p><h2>Generate Tests from a Requirements Assessment<a name="7"></a></h2><p>An alternative way to express the expected requirement behavior is with a <b>Test Assessment</b> block.  We will be using same "Disengage With Brake" requirement and build a requirements based model with a <b>Test Assessment</b> block for test generation.  This will be a text based approach.</p><p>Do the following:</p><p>1.  Open the <b>CruiseControl_TestGen_DisengageWithBrake_Seq</b> model &#8211; <b><a href="matlab:D0_TestGenDisengageWithBrakeSeq()">click here</a></b></p><p>2.  Open the "Test Assessment" block in the harnessDisengage with Brake" requirement in the editor.</p><p><img vspace="5" hspace="5" src="Step_05_FuncGenReqSeq.png" alt=""> </p><p>Notice the use of <b>sldv</b> and other commands in the assessment:</p><div><ul><li><b>sldv.condition(Speed&gt;= 20 &amp;&amp; Speed&lt;=90)</b> will constrain the generated test case to have the "Speed" input signal to be between (20) and (90).</li><li><b>sldv.test(TO_DisengageWithBrake)</b> is a test objective with the same function as the <b>Test Objective</b> in the previous section where a test case will be generated that will satisfy this objective for at least one sample.</li><li>The test objective "Brake &amp;&amp; ~BrakeLast &amp;&amp; engagedLast" which is equivalent to the previous section objective where the brake has just been applied and the cruise control for the last sample was engaged.</li><li>The test "verfiy" signal is checked when the test objective has been satisfied, otherwise it is not evaluated.</li><li>Lastly there are commands to save the previous state of the "Brake" and "engaged" signal</li></ul></div><p>3.  Generate the test case by the menu selection <b>Analysis/Design Verifier/Generate Tests/Model</b></p><p><img vspace="5" hspace="5" src="Step_05_FuncGenSeqResult.png" alt=""> </p><p>4.  Select <b>Export test cases to Simulink Test</b>.  Use the default settings to complete the export.</p><p>5.  Run the test in the <b>Test Manager</b> and analyze the results.</p><p><img vspace="5" hspace="5" src="Step_05_FuncGenSeqSLTestSDI.png" alt=""> </p><p>Notice the <b>Test Manager</b> shows a "verify" signal with a value of (untested) until the last point shows (pass).</p><p>6.  Make the menu selection <b>Analysis/Design Verifier/Prove Property/Model</b> to perform "Property Proving" using the "verify" as a property proof.</p><p>The <b>Test Assessment</b> based method provides a text based approach to authoring the requirements models.  Which method text model or block based is a matter of user preference.  The block based method offers the benefits of showing a requirements in a block diagram which may offer an more intuitive presentation of the requirements model.  To support the generation of test cases for multiple requirements both methods would likely benefit from the use of variants.</p><p>For the block based approach there are additional blocks that help with the construction of the requirements models:</p><div><ul><li>Open the base Model Verification library &#8211; <b><a href="matlab:open_system('simulink');open_system('simulink/Model%20Verification')">click here</a></b></li></ul></div><div><ul><li>Open the <b>sldv</b> property examples library &#8211; <b><a href="matlab:open_system('sldvdemo_properties_spec')">click here</a></b></li></ul></div><div><ul><li>Open the <b>sldv</b> temporal operator examples library &#8211; <b><a href="matlab:open_system('sldv_examples_to')">click here</a></b></li></ul></div><div><ul><li>Open the <b>design optimization</b> verification library &#8211; <b><a href="matlab:open_system('sdolib');open_system('sdolib/Model%20Verification')">click here</a></b></li></ul></div><h2>Summary<a name="8"></a></h2><p>We demonstrated the "top-it-off" workflow where coverage information from the partial coverage, requirements based test cases were combined with the generated test cases to achieve 100% coverage.  <b>Simuink Design Verifier</b> was configured to use the partial coverage data file to only create test cases necessary to achieve the 100% coverage goal.  Expected outputs were also included with the generated test cases that were the measured outputs with the current version of the "Cruise Control" module. We now have a test suite with 100% coverage, test vectors that can be used to evaluate newer versions of the implementation.  In the next step we will use the complete <b>Simulink Test</b> suite to verify the outputs of the model match the generated code in the next step <b>Code Verification</b>.</p><p>One last item to note is there will be some additional management of the "Generated" test case to include the external files for the inputs and baseline criteria.</p><p>Next we showed requirements based test generation using a block approach using some of the additional blocks provided by <b>Design Verifier</b>.  And then we showed a text based approach using <b>Test Assessments</b> a feature provided by <b>Simulink Test</b>.  Both approaches provided the ability to reuse for <b>Property Proving</b> to "prove" the implementation behavior will comply to the requirement.</p><div><ul><li>When you are finished, close all models and files - or <b><a href="matlab:bdclose('all');">click here</a></b>.</li><li>Go to <b>Step 6: Component Code Verification</b> - <b><a href="Step_06.html">click here</a></b>.</li></ul></div><p class="footer"><br><a href="http://www.mathworks.com/products/matlab/">Published with MATLAB&reg; R2018b</a><br></p></div><!--
##### SOURCE BEGIN #####
%% Step 5: Test Case Generation 
% 
%% Introduction
%
% In this section, we are going to collect coverage on our cruise control
% module using the test harness model with the initial set of input test 
% vectors.  The coverage analysis from running these test vector provided
% high but partial model coverage.  As before we are going to run these 
% initial test cases in *Simulink Test* but this time we will ask 
% *Simulink Design Verifier* to generate the test cases to cover the 
% missing coverage objectives.  The test generation tool can ignore the 
% already satisfied objectives and only create test cases that when 
% combined with the inital test vectors provide complete model coverage.  
% This is typically called a "top-it-off" test generation workflow.  In 
% this step we will see how the integration of *Simulink Test* and 
% *Simulink Design Verifier* supports this workflow.
%
% This step is a prerequisite to the next step *Code Verification* where 
% we will perform "equivalence testing".  In order to verify equivalent
% behaviors between the model and the generated code, the comparison 
% analysis requires 100% model coverage, input test vectors. In addition we
% will verify that the code generation did not create unintended
% functionality by comparing model and code coverage.  With 100% model
% coverage, sections of the code with incomplete coverage may indicate 
% possible unintended functionality.
%
% In our previous step we were able to analyze the coverage 
% results on the model, identify missing requirements and incomplete test 
% cases to achieve 100% coverage.  On a more complex model or within a 
% tight development schedule, a "top-it-off" workflow is often used to 
% quickly create a set of test vectors to support equivalence testing.  A
% side benefit to the test generation is to identify dead logic when the
% test generation procedure reports certain coverage objectives as 
% "unsatisfiable" although this is now typically done with the *Design
% Error Detection* feature of *Simulink Design Verifier*.
%
% In addition we will look at generating tests based on requirements.  For
% these tests the functional behavior of the requirements is modeled.
% *Simulink Design Verifier* will generate a test based on the conditions
% of the functional behavior model.  The technique can be applied to
% critical behaviors or to completely demonstrate behavior compliance to
% the requirements.  Generally the additional effort to create the behavior
% models is best applied to the critical or safety related behaviors.
%
%% Verification and Validation Tools Used
% * Simulink Test
% * Simulink Verification and Validation
% * Simulink Design Verifier
%
%% Collect Initial Model Coverage with Simulink Test
% As we said in the introduction we will be using our partial coverage test
% harness model with the (14) test cases from the previous step but with
% a "Cruise Control" model with all the previous bug fixes.  *Simulink Test*
% will be used to run the tests and collect the intial coverage.
%
% Do the following:
% 
% 1. Load the *Simulink Test*, *Test Manager* configuration file named 
% "topItOff" - *<matlab:C5_TopItOffTestGen; click here>*.
%
% 2. Navigate to the test case "Partial Cov" and analyze the configuration.
%
% <<Step_05_TestMgrTopItOff.png>>
%
% 3. We will use the *Test Manager* to open the "Cruise Control" model with 
% all the bug fixes including the logic issue we found in the last step.  
% Open the "Cruise Control" model by clicking the "arrow" next to the
% *Model* field.
%
% We will be generating test cases on a production version that is ready 
% for code generation.  Typically test cases are generated on a production
% version to support equivalence testing to verify the behavior of the 
% model and code match.  We will run the equivalence test in the next  
% step *Component Code Verification*.  For now review the production version
% to confirm the model contains the bug fixes.
%
% <<Step_05_NoDesignIssue.png>>
%
% 4. Open the test harness "CruiseControl_Harness_SB" containing the 
% partial coverage test vectors by clicking on the "arrow" next to the 
% *Harness* field. 
%
% <<step_05_PartialCovHarness.png>>
%
% Confirm the test harness model contains the partial coverage test
% vectors.
%
% <<step_05_PartialCovSigBldr.png>>
%
% The *Test Manager* configuration is very similar to what was configured 
% manually in the previous step *Testing by Simulation*.  It has been 
% configured for you so we can focus on test generation for coverage.  The 
% details of how to configure the *Test Manager* can be found in the 
% previous step *Testing by Simulation*.  Let's review the test case 
% configuration.  
%
% 5. Return to the *Test Manager* to check that the *Signal Builder* test 
% cases have been configured to be used in the *Inputs* section.  
%
% <<Step_05_InputsSetupInitCov.png>>
%
% 6. Examine the *Iterations* configuration to see that we will iterate 
% across the (14) *Signal Builder* test cases.  Notice the *Scripted
% Iterations* method is being so select *Show Iterations* to confirm the
% (14) initial test cases will be executed.
%
% <<Step_05_ItersSetupInitCov.png>>
%
% 7. Check the file level *coverage settings* configuration to see that 
% decision, condition and MCDC coverage has been enabled at the file level 
% and the test case level.
%
% <<Step_05_TestMgrCovSettings.png>>
%
% 8. We are ready to execute the partial coverage, test vectors and collect
% the coverage results.  At the "Partial Cov" test case level, select *Run*.
%
% <<Step_05_RunInitCov.png>>
%
% From the results, we can see that (14) test cases have been successfully
% executed and the model coverage has been measured.
%
% <<Step_05_PartialCovResults.png>>
%
% This completes the initial model coverage collection task.  Next we will
% generate test vectors for complete coverage.
% 
%% Generate Test Cases for Missing Model Coverage
%
% The coverage results from the initial test vectors were:
%
% * 92% Decision Coverage
% * 81% Condition Coverage
% * 63% MCDC Coverage
%
% *Simulink Design Verifier* will be used to create additional test cases
% that will provide the missing coverage.  When the generated test cases
% are executed along with the initial test cases the coverage analysis will
% show 100% model coverage.  Now let’s see how the integration of 
% *Simulink Design Verifier* facilitates the generation of these 
% "top-it-off" test cases.
%
% From *Results Explorer* pane in the *Test Manager* we will begin the 
% "top-it-off" test case generation.
%
% 1.  *Click* on *Add Tests for Missing Coverage*
%
% <<Step_05_AddMissingTests.png>>
%
% 2.  Populate the *Add Tests for Missing Coverage* dialog as shown.
%
% <<Step_05_AddMissingTestsDlg.png>>
%
% A dialog popups to show the status of the test case generation.  
% *Simulink Design Verifier* is only generating test cases for the missing 
% coverage by using the coverage results from the *Test Manager*.
%
% <<Step_05_DV_ProgressDlg.png>>
%
% When the analysis has completed, *Simulink Design Verifier* has generated
% test cases for the (15) coverage objectives, and exports the test cases 
% to the *Test Manager* in the new test case "New Test Case 1".
%
% <<Step_05_ExpTestCasesInTestMgr.png>>
%
% 3.  Cut and paste the exported test cases into the original test suite.
%
% 4.  Rename exported test to "Generated"
%
% <<Step_05_ExpTestCasesMovedRenamed.png>>
%
% A new test case "Generated" has been created in the *Test Manager*.  In 
% the next section we will look at the contents of the generated test cases
% and run all the test cases to demonstrate full coverage.
%
%% Run Full Coverage Test Cases in Simulink Test
%
% In the *Test Manager* we have a new test case that was created during 
% the export of the generated test cases to *Simulink Test*.  Let's examine
% the contents of the generated test case "Generated" and then run all the 
% tests for the module:
%
% 1.  Navigate to the test case "Generated".  The test case is a 
% *Baseline Test* type so the evaluation will be based on external files 
% for the comparison.  
%
% <<Step_05_TestCaseCfgHarness.png>>
%
% 2.  Looking at the *SYSTEM UNDER TEST* section, the *Model* is 
% "CruiseControl" as expected but the *Harness* is the newly, generated 
% "CruiseControl_Harness1".  The harness is a *Simulink Test* harness 
% that is now contained in the model.
%
% The test case is using the *Simulink Design Verifier* the (8) generated 
% test cases along with the expected results. 
%
% 3.  Expand the *INPUTS* section to see the *EXTERNAL INPUTS* section has
% been populated with test cases from the *Simulink Design Verifier* data file
% "CruiseControl_sldvdata.mat".
% 
% <<Step_05_TestCaseCfgInputsBaseline.png>>
%
% 4. Expand the *BASELINE CRITERIA* section to see the test case has been
% populated with expected outputs from the test generation data file.
% These were included in the export as a result of configuring *Design Verifier* 
% to *Include expected output values*. 
%
% <<Step_05_TestGenCfgExpValues.png>>
%
% These values are outputs that were measured by executing the 
% generated test case inputs.  So these expected output test vectors may 
% not be useful for checking the current model but will be useful for 
% checking the generated code in the next step and for checking later 
% versions of the model.
%
% 5. Look at the *ITERATIONS* section to see the test case will be
% iterated over the input data files and the expected output data files.
% Optionally check *Run test iterations in fast restart*.
%
% <<Step_05_TestGenCfgIterations.png>>
%
% We will be executing the original, partial coverage test vectors along
% with the generated test vectors to produce complete coverage.  As a
% personal choice, the generated test cases were copied into the
% original test suite so all the test will be executed from a single suite.
% This way all the test cases for the "Cruise Control" model will be 
% contained in a single test file.  But you may choose to have these in
% separate files depending on your process.
%
% 6.  Run the tests from the "topItOff/Test Suite" level to measure the 
% expected full coverage.
%
% <<Step_05_RunReqAndGenTests.png>>
%
% In the *Results and Artifacts* pane the test results show 100% coverage.
% Notice there are the original (14) requirements based test vectors along
% with the (8) coverage based generated test vectors.  Depending on the 
% available development time, the generated test cases may be used as hints
% that can be used to reverse engineer functional test cases from them, 
% associate them with a requirement and create expected outputs.  
% Generating test cases for functional objectives is discussed in the nest
% section.  But for the purpose of equivalence testing we are ready  
% for the next step *Component Verification* with these structural test
% cases.
%
% <<Step_05_TestMgrGenTestResults.png>>
%
%% Generate a Test from a Requirements Model 
% 
% In this section we will demonstrate the use of the *Test Generation*
% feature for functional tests.  The behavior as described in the
% requirements is expressed as a model with two parts:  (1) an expected 
% action or output behavior based on (2) when a condition or event occurs.  
% The condition can include a logical sequence of current and past inputs, 
% and often past outputs.  The expected action or output behavior model 
% will also include current outputs.
%
% For the first example we will return to the "Disengage With Brake"
% requirement and build a requirements based model for test generation.
%
% Do the following:
%
% 1.  Open the *CruiseControl_TestGen_DisengageWithBrake* model – 
% *<matlab:D0_TestGenDisengageWithBrake() click here>*
%
% 2.  Navigate to the "Disengage with Brake" requirement in the editor.
%
% <<Step_05_FuncGenReq.png>>
% 
% Notice this requirement is derived from a higher level safety
% requirement, linked to a test case and implemented by the transition
% "[Brake||Speed>maxtspeed||Speed<mintspeed]".
%
% 3.  Select the implementation link to navigate to the model transition.
%
% <<Step_05_FuncGenSF.png>>
%
% 4.  Determine the requirements linked to this transition.
%
% Notice that based on the implementation and another requirement, a low or
% high speed can also execute the transition.
%
% 5.  Navigate to the *CruiseControl_TestGen_DisengageWithBrake* model, and
% open the "Verification Subsystem".
%
% <<Step_05_FuncGenReqTO.png>>
%
% The highlighted blocks is the part of the requirements model that will be
% used by *Design Verifier* to generate the test.  The test case will 
% create the condition described in the requirement.  The "DisengageWithBrake_Test"
% is a *Test Objective* block that will instruct the tool to create a test 
% case where its input is true for at least one time step.  Study the input
% signal to the test objective to understand how this represents the
% condition of the requirement.
%
% The test case generation includes a block to constrain "Speed" signal to
% be between the limits of (20) and (90).  The generated test will not
% include a "Speed" signal that could cause a disengagement to cloud the
% applicability of the test case.
%
% <<Step_05_FuncGenReqTC.png>>
%
% 6.  Generate the test case by the menu selection *Analysis/Design
% Verifier/Generate Tests/Model*
%
% <<Step_05_FuncGenResult.png>>
%
% 7.  Select *Export test cases to Simulink Test*.  Use the default
% settings to complete the export.
%
% <<Step_05_FuncGenSLTestExport.png>>
%
% 8.  Run the test in the *Test Manager* and analyze the results.
%
% <<Step_05_FuncGenSLTestSDI.png>>
%
% Notice the *Test Manager* shows a pass with a "verify" signal.  How was
% the "verify signal" generated?  
%
% <<Step_05_FuncGenReqVerify.png>>
%
% The "verify" signal was generated with highlighted blocks in the
% "Verification Subsystem".  Looking at the "Implies" block:
%
% * When the condition at input "A" is true
% * Then the action (expected output) at "B" is expected to be true
% * And the output result is true 
% * When the condition at input "A" is false then the output result is
% "true" regardless of whether "B" is true or false
%
% The "Implies" block output is connected to an "assertion" block that 
% creates the "verify" signal.  The "assertion" provided a test result assessment to the *Test Manager*
% based on the single generated test case.  
%
% Are there other test cases that could be generated based on the "test 
% objective" but fail the "assertion"?  Meaning the implementation will not
% produce the expected output or action?  *Design Verifier* provides 
% another function known as *Property Proving* to prove there are no test 
% case inputs that can fail the "assertion".  If *Property Proving* can 
% falsify the "assertion" or proof then it will create a test case for the 
% user to debug the implementation.  In a later step we will go into more 
% detail but for now we will perform the *Property Proving* to see if there
% is a test case that can show the implementation will not meet the requirement.
%
% 9.  Make the menu selection *Analysis/Design Verifier/Prove Property/Model*
% to perform the analysis.
%
% <<Step_05_FuncGenProveResult.png>>
%
% On the model the "assertion" block is colored "green" to indicate the
% property proof was satisfied (not falsified).  In the later step we will
% show how property proving can be used to iteratively improve the
% robustness of the design.
%
% <<Step_05_FuncGenReqProve.png>>
%
%%  Generate Tests from a Requirements Assessment
% 
% An alternative way to express the expected requirement behavior is with a
% *Test Assessment* block.  We will be using same "Disengage With Brake"
% requirement and build a requirements based model with a *Test Assessment*
% block for test generation.  This will be a text based approach.
%
% Do the following:
%
% 1.  Open the *CruiseControl_TestGen_DisengageWithBrake_Seq* model – 
% *<matlab:D0_TestGenDisengageWithBrakeSeq() click here>*
%
% 2.  Open the "Test Assessment" block in the harnessDisengage with Brake" requirement in the editor.
%
% <<Step_05_FuncGenReqSeq.png>>
%
% Notice the use of *sldv* and other commands in the assessment:
%
% * *sldv.condition(Speed>= 20 && Speed<=90)* will constrain the generated
% test case to have the "Speed" input signal to be between (20) and (90).
% * *sldv.test(TO_DisengageWithBrake)* is a test objective with the same
% function as the *Test Objective* in the previous section where a test
% case will be generated that will satisfy this objective for at least one
% sample.
% * The test objective "Brake && ~BrakeLast && engagedLast" which is
% equivalent to the previous section objective where the brake has just
% been applied and the cruise control for the last sample was engaged.
% * The test "verfiy" signal is checked when the test objective has been
% satisfied, otherwise it is not evaluated.
% * Lastly there are commands to save the previous state of the "Brake" and
% "engaged" signal
%
% 3.  Generate the test case by the menu selection *Analysis/Design
% Verifier/Generate Tests/Model*
%
% <<Step_05_FuncGenSeqResult.png>>
%
% 4.  Select *Export test cases to Simulink Test*.  Use the default
% settings to complete the export.
%
% 5.  Run the test in the *Test Manager* and analyze the results.
%
% <<Step_05_FuncGenSeqSLTestSDI.png>>
%
% Notice the *Test Manager* shows a "verify" signal with a value of 
% (untested) until the last point shows (pass).
%
% 6.  Make the menu selection *Analysis/Design Verifier/Prove Property/Model*
% to perform "Property Proving" using the "verify" as a property proof.
%
% The *Test Assessment* based method provides a text based approach to
% authoring the requirements models.  Which method text model or block
% based is a matter of user preference.  The block based method offers the
% benefits of showing a requirements in a block diagram which may offer an
% more intuitive presentation of the requirements model.  To support 
% the generation of test cases for multiple requirements both methods 
% would likely benefit from the use of variants.
%
% For the block based approach there are additional blocks that help with
% the construction of the requirements models:  
%
% * Open the base Model Verification library –
% *<matlab:open_system('simulink');open_system('simulink/Model%20Verification') click here>* 
%
% * Open the *sldv* property examples library –
% *<matlab:open_system('sldvdemo_properties_spec') click here>*  
%
% * Open the *sldv* temporal operator examples library –
% *<matlab:open_system('sldv_examples_to') click here>* 
%
% * Open the *design optimization* verification library –
% *<matlab:open_system('sdolib');open_system('sdolib/Model%20Verification') click here>* 
%
%% Summary
% 
% We demonstrated the "top-it-off" workflow where coverage information from
% the partial coverage, requirements based test cases were combined with the
% generated test cases to achieve 100% coverage.  *Simuink Design Verifier* 
% was configured to use the partial coverage data file to only create test 
% cases necessary to achieve the 100% coverage goal.  Expected outputs were
% also included with the generated test cases that were the measured 
% outputs with the current version of the "Cruise Control" module. We now 
% have a test suite with 100% coverage, test vectors that can be used to 
% evaluate newer versions of the implementation.  In the next step we will 
% use the complete *Simulink Test* suite to verify the outputs of the model
% match the generated code in the next step *Code Verification*.
%
% One last item to note is there will be some additional management of the 
% "Generated" test case to include the external files for the inputs and
% baseline criteria.
%
% Next we showed requirements based test generation using a block approach 
% using some of the additional blocks provided by *Design Verifier*.  And
% then we showed a text based approach using *Test Assessments* a feature
% provided by *Simulink Test*.  Both approaches provided the ability to
% reuse for *Property Proving* to "prove" the implementation behavior will
% comply to the requirement.
%
% * When you are finished, close all models and files - or
% *<matlab:bdclose('all'); click here>*.
% * Go to *Step 6: Component Code Verification* - *<Step_06.html click here>*.
%

##### SOURCE END #####
--></body></html>